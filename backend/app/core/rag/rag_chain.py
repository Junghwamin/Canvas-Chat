from typing import AsyncGenerator, List, Dict, Optional, Tuple
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.documents import Document
from app.core.config import settings
import os

# Few-Shot ì˜ˆì‹œ
FEW_SHOT_EXAMPLES = """
## ì¢‹ì€ ë‹µë³€ ì˜ˆì‹œ

### ì˜ˆì‹œ 1:
ì§ˆë¬¸: "ì´ ë¬¸ì„œì—ì„œ ë§í•˜ëŠ” í•µì‹¬ ê¸°ëŠ¥ì€ ë¬´ì—‡ì¸ê°€ìš”?"
ì‚¬ê³  ê³¼ì •:
1. ë¨¼ì € ë¬¸ì„œì—ì„œ "í•µì‹¬ ê¸°ëŠ¥", "ì£¼ìš” ê¸°ëŠ¥", "feature" ë“±ì˜ í‚¤ì›Œë“œë¥¼ ì°¾ìŠµë‹ˆë‹¤.
2. í•´ë‹¹ ì„¹ì…˜ì˜ ë‚´ìš©ì„ ìš”ì•½í•©ë‹ˆë‹¤.
3. ì¤‘ìš”ë„ì— ë”°ë¼ ì •ë¦¬í•˜ì—¬ ë‹µë³€í•©ë‹ˆë‹¤.
ë‹µë³€: "ë¬¸ì„œì— ë”°ë¥´ë©´ í•µì‹¬ ê¸°ëŠ¥ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: ì²«ì§¸, ... ë‘˜ì§¸, ... ì…‹ì§¸, ..."

### ì˜ˆì‹œ 2:
ì§ˆë¬¸: "ì´ì „ ì§ˆë¬¸ê³¼ ê´€ë ¨í•´ì„œ ë” ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”"
ì‚¬ê³  ê³¼ì •:
1. ì´ì „ ëŒ€í™” ë‚´ìš©ì„ í™•ì¸í•©ë‹ˆë‹¤.
2. ì´ì „ì— ë…¼ì˜ëœ ì£¼ì œì™€ ê´€ë ¨ëœ ì¶”ê°€ ì •ë³´ë¥¼ ë¬¸ì„œì—ì„œ ì°¾ìŠµë‹ˆë‹¤.
3. ë§¥ë½ì„ ìœ ì§€í•˜ë©° ìƒì„¸í•œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
ë‹µë³€: "ì•ì„œ ë§ì”€ë“œë¦° [ì´ì „ ì£¼ì œ]ì— ëŒ€í•´ ë” ìì„¸íˆ ì„¤ëª…ë“œë¦¬ë©´..."
"""

# Chain of Thought í”„ë¡¬í”„íŠ¸ (ì¶œì²˜ í¬í•¨)
COT_SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ì œê³µëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ê³ ê¸‰ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

## ì‚¬ê³  ë°©ì‹ (Chain of Thought)
ë‹µë³€í•˜ê¸° ì „ì— ë‹¤ìŒ ë‹¨ê³„ë¥¼ ë”°ë¥´ì„¸ìš”:
1. **ì§ˆë¬¸ ë¶„ì„**: ì‚¬ìš©ìê°€ ì •í™•íˆ ë¬´ì—‡ì„ ë¬»ê³  ìˆëŠ”ì§€ íŒŒì•…
2. **ë§¥ë½ í™•ì¸**: ì´ì „ ëŒ€í™” ë‚´ìš©ì´ ìˆë‹¤ë©´ ì°¸ê³ í•˜ì—¬ ë§¥ë½ ìœ ì§€
3. **ë¬¸ì„œ ê²€ìƒ‰**: ì œê³µëœ Contextì—ì„œ ê´€ë ¨ ì •ë³´ ì°¾ê¸°
4. **ì •ë³´ ì¢…í•©**: ì°¾ì€ ì •ë³´ë¥¼ ë…¼ë¦¬ì ìœ¼ë¡œ ì •ë¦¬
5. **ë‹µë³€ ìƒì„±**: ëª…í™•í•˜ê³  êµ¬ì¡°í™”ëœ ë‹µë³€ ì œê³µ

## ë°ì´í„° ë¶„ì„ ë° í†µê³„ ê´€ë ¨ ì§ˆë¬¸ ì‹œ íŠ¹ë³„ ì§€ì¹¨
ì‚¬ìš©ìê°€ ë°ì´í„° ì „ì²˜ë¦¬, ê²°ì¸¡ì¹˜, ì½”ë”© ë°©ì‹ ë“±ì— ëŒ€í•´ ì§ˆë¬¸í•  ê²½ìš°:
1. **íŒŒì¼ ê°„ êµì°¨ ê²€ì¦**: ì œê³µëœ 'ì„¤ë¬¸ì§€(Questionnaire)', 'ì—°êµ¬ê³„íšì„œ', 'ì½”ë”© ë°ì´í„°(Excel/CSV)' íŒŒì¼ë“¤ì„ ì„œë¡œ ëŒ€ì¡°í•˜ì„¸ìš”.
2. **ê²°ì¸¡ì¹˜ ì›ì¸ íŒŒì•…**: 
   - ë‹¨ìˆœí•œ "ì‘ë‹µ ì—†ìŒ"ì¸ì§€, "ì¡°ê±´ë¶€ ë¬¸í•­ì— ë”°ë¥¸ ìì—°ìŠ¤ëŸ¬ìš´ ê²°ì¸¡(Valid Skip)"ì¸ì§€ ì„¤ë¬¸ ë¡œì§ì„ í†µí•´ í™•ì¸í•˜ì„¸ìš”.
   - ì˜ˆ: "í¡ì—° ê²½í—˜ ì—†ìŒ" -> "í•˜ë£¨ í¡ì—°ëŸ‰" ë¬¸í•­ì€ ê²°ì¸¡ì´ ì •ìƒì…ë‹ˆë‹¤.
3. **êµ¬ì²´ì  í•´ê²°ì±… ì œì‹œ**:
   - ì¼ë°˜ì ì¸ êµê³¼ì„œì  ë‹µë³€(ì‚­ì œ/í‰ê· ëŒ€ì²´ ë“±)ì„ ì§€ì–‘í•˜ì„¸ìš”.
   - í•´ë‹¹ ë°ì´í„°ì˜ ë§¥ë½ì— ë§ëŠ” êµ¬ì²´ì  ì²˜ë¦¬ë¥¼ ì œì•ˆí•˜ì„¸ìš”. (ì˜ˆ: "í•´ë‹¹ ë³€ìˆ˜ëŠ” ì¡°ê±´ë¶€ ë¬¸í•­ì´ë¯€ë¡œ ê²°ì¸¡ì¹˜ë¥¼ 0 ë˜ëŠ” -1ë¡œ ì½”ë”©í•˜ì—¬ ë¶„ì„ì— í¬í•¨í•˜ì„¸ìš”.")

## ë‹µë³€ ê·œì¹™
- ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê²Œ ë‹µë³€
- ì´ì „ ëŒ€í™”ì˜ ë§¥ë½ì„ ë°˜ë“œì‹œ ê³ ë ¤
- ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì†”ì§íˆ "ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€
- ë‹µë³€ ì‹œ ì–´ëŠ ë¬¸ì„œì—ì„œ ì •ë³´ë¥¼ ê°€ì ¸ì™”ëŠ”ì§€ ëª…ì‹œ (ì˜ˆ: "[ë¬¸ì„œ 1]ì— ë”°ë¥´ë©´...")
- ë³µì¡í•œ ë‹µë³€ì€ ë²ˆí˜¸ë‚˜ ë¶ˆë¦¿ í¬ì¸íŠ¸ë¡œ êµ¬ì¡°í™”

{few_shot_examples}
"""


class RAGChain:
    def __init__(self, retriever):
        self.retriever = retriever
        self.llm = ChatOpenAI(
            openai_api_key=settings.OPENAI_API_KEY,
            model="gpt-4o",
            temperature=0.1,
            streaming=True
        )
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸
        self.prompt = ChatPromptTemplate.from_messages([
            ("system", COT_SYSTEM_PROMPT),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", """
# ë¬¸ì„œ Context (ì¶œì²˜ í¬í•¨):
{context}

# ì‚¬ìš©ì ì§ˆë¬¸:
{question}

# ë‹µë³€ ì‹œ ë°˜ë“œì‹œ ì–´ëŠ ë¬¸ì„œì—ì„œ ì •ë³´ë¥¼ ê°€ì ¸ì™”ëŠ”ì§€ ì–¸ê¸‰í•´ì£¼ì„¸ìš”:""")
        ])
        
        # ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ë‹¨ìˆœ í”„ë¡¬í”„íŠ¸
        self.simple_prompt = ChatPromptTemplate.from_template(
            """ë‹¹ì‹ ì€ ì œê³µëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
            ë¬¸ì„œì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ìƒì„¸í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
            ë‹µë³€ ì‹œ ì–´ëŠ ë¬¸ì„œì—ì„œ ì •ë³´ë¥¼ ê°€ì ¸ì™”ëŠ”ì§€ ì–¸ê¸‰í•´ì£¼ì„¸ìš”.
            ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ "ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•´ì£¼ì„¸ìš”.
            
            # Context (ì¶œì²˜ í¬í•¨):
            {context}
            
            # Question:
            {question}
            
            # Answer:"""
        )

    def _format_docs_with_sources(self, docs: List[Document]) -> Tuple[str, List[Dict]]:
        """ë¬¸ì„œë¥¼ í¬ë§·í•˜ê³  ì¶œì²˜ ì •ë³´ ë°˜í™˜"""
        if not docs:
            return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", []
        
        sources = []
        formatted_parts = []
        
        for i, doc in enumerate(docs):
            # ë©”íƒ€ë°ì´í„°ì—ì„œ íŒŒì¼ ì •ë³´ ì¶”ì¶œ
            metadata = doc.metadata or {}
            full_path = metadata.get("source", metadata.get("file_path", "Unknown"))
            
            # íŒŒì¼ëª…ë§Œ ì¶”ì¶œ
            source_file = full_path
            if full_path and full_path != "Unknown":
                source_file = os.path.basename(full_path)
            
            page = metadata.get("page", None)
            chunk_id = metadata.get("chunk_id", i + 1)
            
            # ì¶œì²˜ ì •ë³´ ì €ì¥ (íŒŒì¼ ê²½ë¡œ í¬í•¨)
            source_info = {
                "document": source_file,
                "file_path": full_path,  # ì „ì²´ ê²½ë¡œ ì¶”ê°€
                "page": page,
                "chunk": chunk_id,
                "excerpt": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content
            }
            sources.append(source_info)
            
            # í¬ë§·ëœ ì»¨í…ìŠ¤íŠ¸
            source_label = f"ğŸ“„ **[ë¬¸ì„œ {i+1}]** {source_file}"
            if page:
                source_label += f" (í˜ì´ì§€ {page})"
            
            formatted_parts.append(f"{source_label}\n{doc.page_content}")
        
        return "\n\n---\n\n".join(formatted_parts), sources

    def _format_docs(self, docs):
        """ë‹¨ìˆœ í¬ë§· (í˜¸í™˜ì„±ìš©)"""
        context, _ = self._format_docs_with_sources(docs)
        return context
    
    def _format_chat_history(self, history: List[Dict]) -> List:
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ LangChain ë©”ì‹œì§€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        messages = []
        for msg in history:
            if msg.get("role") == "user":
                messages.append(HumanMessage(content=msg.get("content", "")))
            elif msg.get("role") == "assistant":
                messages.append(AIMessage(content=msg.get("content", "")))
        return messages

    def get_chain(self):
        """ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ë‹¨ìˆœ ì²´ì¸"""
        rag_chain = (
            {"context": self.retriever | self._format_docs, "question": RunnablePassthrough()}
            | self.simple_prompt
            | self.llm
            | StrOutputParser()
        )
        return rag_chain

    async def astream_answer(self, question: str) -> AsyncGenerator[str, None]:
        """ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ìŠ¤íŠ¸ë¦¬ë°"""
        chain = (
            {"context": self.retriever | self._format_docs, "question": RunnablePassthrough()}
            | self.simple_prompt
            | self.llm
            | StrOutputParser()
        )
        
        async for chunk in chain.astream(question):
            yield chunk

    async def astream_answer_with_history(
        self, 
        question: str, 
        chat_history: Optional[List[Dict]] = None
    ) -> AsyncGenerator[str, None]:
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ì™€ CoTë¥¼ í¬í•¨í•œ ê³ ê¸‰ ìŠ¤íŠ¸ë¦¬ë°"""
        history = chat_history or []
        formatted_history = self._format_chat_history(history)
        
        # ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ (ìµœê·¼ ëŒ€í™” ë‚´ìš©ë„ ì¿¼ë¦¬ì— í¬í•¨)
        enhanced_query = question
        if history:
            recent_context = " ".join([
                msg.get("content", "")[:100] 
                for msg in history[-3:]
            ])
            enhanced_query = f"{recent_context} {question}"
        
        # ë¬¸ì„œ ê²€ìƒ‰
        docs = self.retriever.invoke(enhanced_query)
        context, sources = self._format_docs_with_sources(docs)
        
        # ì²´ì¸ ì‹¤í–‰
        chain = self.prompt | self.llm | StrOutputParser()
        
        async for chunk in chain.astream({
            "context": context,
            "question": question,
            "chat_history": formatted_history,
            "few_shot_examples": FEW_SHOT_EXAMPLES
        }):
            yield chunk
        
        # ë§ˆì§€ë§‰ì— ì¶œì²˜ ì •ë³´ ì¶”ê°€ (íŒŒì¼ ê²½ë¡œ í¬í•¨)
        if sources:
            yield "\n\n---\nğŸ“š **ì¶œì²˜:**\n"
            for i, src in enumerate(sources):
                source_text = f"- **{src['document']}**"
                if src.get('page'):
                    source_text += f" (p.{src['page']})"
                source_text += f"\n  ğŸ“ ê²½ë¡œ: `{src.get('file_path', 'N/A')}`"
                source_text += f"\n  ğŸ“ ë°œì·Œ: \"{src['excerpt'][:100]}...\"\n"
                yield source_text

    async def get_sources(self, question: str) -> List[Dict]:
        """ì§ˆë¬¸ì— ëŒ€í•œ ì†ŒìŠ¤ ë¬¸ì„œ ì •ë³´ë§Œ ë°˜í™˜"""
        docs = self.retriever.invoke(question)
        _, sources = self._format_docs_with_sources(docs)
        return sources
